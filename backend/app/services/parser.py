import asyncio
import random
import logging
from typing import Set, Optional
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs
import urllib.parse

from playwright.async_api import async_playwright, Page, BrowserContext

# ================ CONFIG ================
logging.basicConfig(
    filename="parser.log",
    filemode="a",
    level=logging.INFO,
    format="%(asctime)s - [%(levelname)s] - %(message)s",
)

# User-Agent pool –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
]

# –ß—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ (–ø–µ—Ä–µ–∫—É–ø—â–∏–∫–∏, –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã, –Ω–µ B2B)
BLACKLIST_DOMAINS = {
    "avito.ru",
    "ozon.ru",
    "wildberries.ru",
    "youla.ru",
    "lamoda.ru",
    "ebay.com",
    "aliexpress.com",
    "yandex.ru",
    "google.com",
    "2gis.ru",
    "dzen.ru",
    "vk.com",
    "facebook.com",
    "instagram.com",
    "youtube.com",
    "pinterest.com",
    "reddit.com",
    "wikipedia.org",
    "twitter.com",
    "tiktok.com",
}

BLACKLIST_KEYWORDS = [
    "forum",
    "blog",
    "wiki",
    "stackoverflow",
    "habr.com",
    "pikabu.ru",
    "livejournal",
]

REDIRECTS_TO_CLEAN = {
    "yandex.ru/clck/",
    "safebrowsing",
}

# ================ HELPERS ================

def is_blacklisted(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤ —á—ë—Ä–Ω–æ–º —Å–ø–∏—Å–∫–µ –ª–∏ –¥–æ–º–µ–Ω"""
    domain = urlparse(url).netloc.lower()
    url_lower = url.lower()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–æ–≤
    for bl in BLACKLIST_DOMAINS:
        if bl in domain:
            return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    for kw in BLACKLIST_KEYWORDS:
        if kw in url_lower:
            return True
    
    return False


def is_redirect_link(url: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç –ª–∏ —Å—Å—ã–ª–∫–∞"""
    for redir in REDIRECTS_TO_CLEAN:
        if redir in url:
            return True
    return False


def clean_url(url: str) -> Optional[str]:
    """–û—á–∏—â–∞–µ–º URL –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å"""
    if not url:
        return None
    
    # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
    if is_redirect_link(url):
        return None
    
    # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —á—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫
    if is_blacklisted(url):
        logging.info(f"–ò—Å–∫–ª—é—á—ë–Ω –∏–∑ —á—ë—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞: {url}")
        return None
    
    # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (?xxx&xxx)
    clean = url.split("?")[0].split("#")[0]
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ (https –∏ http –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ)
    if clean.startswith("https://"):
        clean = clean.replace("https://", "http://")
    
    return clean if clean.startswith("http") else None


async def human_pause(min_sec: float = 2.0, max_sec: float = 6.0):
    """–ü–∞—É–∑–∞ —Å —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–µ–π (—É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è)"""
    wait_time = random.uniform(min_sec, max_sec)
    await asyncio.sleep(wait_time)


async def human_scroll(page: Page):
    """–ë–æ–ª–µ–µ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–π —Å–∫—Ä–æ–ª–ª–∏–Ω–≥"""
    for _ in range(random.randint(2, 4)):
        direction = random.choice([200, -300, 150])
        await page.mouse.wheel(0, random.randint(direction - 50, direction + 50))
        await human_pause(0.3, 0.8)


async def human_mouse_movement(page: Page):
    """–î–≤–∏–∂–µ–Ω–∏—è –º—ã—à–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
    for _ in range(random.randint(3, 7)):
        x = random.randint(100, 900)
        y = random.randint(100, 700)
        await page.mouse.move(x, y, steps=random.randint(5, 15))
        await human_pause(0.1, 0.4)


async def very_human_behavior(page: Page):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"""
    await human_pause(1.5, 3.5)
    await human_mouse_movement(page)
    await human_pause(0.8, 2.0)
    await human_scroll(page)
    await human_pause(1.5, 4.0)


async def detect_captcha(page: Page, engine_name: str) -> bool:
    """–£–º–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –∫–∞–ø—á–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ URL)"""
    url = page.url.lower()
    
    # 1) –ü—Ä–æ–≤–µ—Ä–∫–∞ URL
    if "captcha" in url or "showcaptcha" in url or "recaptcha" in url:
        logging.warning(f"{engine_name}: –ö–∞–ø—á–∞ (–ø–æ URL)")
        return True
    
    # 2) –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –≤–∏–¥–∏–º–æ–º—É —Ç–µ–∫—Å—Ç—É
    try:
        captcha_text = await page.locator("text=/–∫–∞–ø—á–∞|–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ|–ø—Ä–æ–≤–µ—Ä–∫–∞/i").count()
        if captcha_text > 0:
            logging.warning(f"{engine_name}: –ö–∞–ø—á–∞ (–ø–æ —Ç–µ–∫—Å—Ç—É)")
            return True
    except:
        pass
    
    # 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ø–Ω–¥–µ–∫—Å–∞: –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ = –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–ø—á–∞
    if engine_name == "YANDEX":
        try:
            results = await page.locator("li.serp-item").count()
            if results == 0:
                logging.warning(f"{engine_name}: –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∫–∞–ø—á–∞?)")
                return True
        except:
            pass
    
    # 4) –ü—Ä–æ–≤–µ—Ä–∫–∞ Google: –µ—Å–ª–∏ –Ω–µ—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ = –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–ø—á–∞
    if engine_name == "GOOGLE":
        try:
            results = await page.locator("div.g").count()
            if results == 0:
                logging.warning(f"{engine_name}: –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∫–∞–ø—á–∞?)")
                return True
        except:
            pass
    
    return False


async def wait_for_page_load(page: Page, timeout: int = 15):
    """–ñ–¥—ë–º –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        await page.wait_for_load_state("networkidle", timeout=timeout * 1000)
    except:
        logging.warning(f"Page load timeout ({timeout}s), continuing anyway...")


# ================ PARSERS ================

async def parse_yandex(
    page: Page,
    query: str,
    pages: int,
    collected_links: Set[str],
    max_retries: int = 3,
):
    """–ü–∞—Ä—Å–∏–Ω–≥ –Ø–Ω–¥–µ–∫—Å–∞ —Å —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞–ø—á–∏"""
    
    query_encoded = urllib.parse.quote(query)
    base_url = f"https://yandex.ru/search/?text={query_encoded}"
    
    for attempt in range(max_retries):
        try:
            logging.info(f"YANDEX: –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
            await page.goto(base_url, wait_until="domcontentloaded")
            await wait_for_page_load(page)
            
            if await detect_captcha(page, "YANDEX"):
                if attempt < max_retries - 1:
                    logging.info("YANDEX: –ñ–¥—ë–º 30 —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                    await human_pause(25, 35)
                    continue
                else:
                    logging.error("YANDEX: –ö–∞–ø—á–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return
            
            # –£—Å–ø–µ—à–Ω–æ –∑–∞—à–ª–∏
            break
        except Exception as e:
            logging.error(f"YANDEX: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {e}")
            if attempt < max_retries - 1:
                await human_pause(10, 15)
            else:
                return
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    for page_num in range(1, pages + 1):
        try:
            logging.info(f"YANDEX: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            print(f"üîç –Ø–Ω–¥–µ–∫—Å: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            
            await very_human_behavior(page)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–ø—á—É
            if await detect_captcha(page, "YANDEX"):
                logging.warning("YANDEX: –ö–∞–ø—á–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –≤—ã—Ö–æ–¥–∏–º")
                break
            
            # –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏)
            selectors = [
                "a.Link",
                "a.Link-Item",
                "h2 a",
            ]
            
            links_on_page = set()
            for selector in selectors:
                try:
                    elems = page.locator(selector)
                    count = await elems.count()
                    
                    for i in range(count):
                        try:
                            href = await elems.nth(i).get_attribute("href")
                            cleaned = clean_url(href)
                            if cleaned:
                                links_on_page.add(cleaned)
                        except:
                            pass
                except:
                    pass
            
            collected_links.update(links_on_page)
            logging.info(f"YANDEX: –ù–∞–π–¥–µ–Ω–æ {len(links_on_page)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é
            if page_num < pages:
                try:
                    next_btn = page.locator("a[aria-label='–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞']")
                    if await next_btn.count() > 0:
                        await human_pause(4, 10)
                        await next_btn.click()
                        await wait_for_page_load(page)
                    else:
                        logging.info("YANDEX: –ù–µ—Ç –∫–Ω–æ–ø–∫–∏ '–î–∞–ª–µ–µ', –≤—ã—Ö–æ–¥–∏–º")
                        break
                except Exception as e:
                    logging.error(f"YANDEX: –û—à–∏–±–∫–∞ –∫–ª–∏–∫–∞ - {e}")
                    break
        
        except Exception as e:
            logging.error(f"YANDEX: –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} - {e}")
            continue
    
    logging.info(f"YANDEX: –ò—Ç–æ–≥–æ {len(collected_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")


async def parse_google(
    page: Page,
    query: str,
    pages: int,
    collected_links: Set[str],
    max_retries: int = 3,
):
    """–ü–∞—Ä—Å–∏–Ω–≥ Google —Å —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–∞–ø—á–∏"""
    
    query_encoded = urllib.parse.quote(query)
    base_url = f"https://www.google.com/search?q={query_encoded}&hl=ru&gl=ru"
    
    for attempt in range(max_retries):
        try:
            logging.info(f"GOOGLE: –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
            await page.goto(base_url, wait_until="domcontentloaded")
            await wait_for_page_load(page)
            
            if await detect_captcha(page, "GOOGLE"):
                if attempt < max_retries - 1:
                    logging.info("GOOGLE: –ñ–¥—ë–º 30 —Å–µ–∫ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                    await human_pause(25, 35)
                    continue
                else:
                    logging.error("GOOGLE: –ö–∞–ø—á–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    return
            
            break
        except Exception as e:
            logging.error(f"GOOGLE: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {e}")
            if attempt < max_retries - 1:
                await human_pause(10, 15)
            else:
                return
    
    # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    for page_num in range(1, pages + 1):
        try:
            logging.info(f"GOOGLE: –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            print(f"üîç Google: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}")
            
            await very_human_behavior(page)
            
            if await detect_captcha(page, "GOOGLE"):
                logging.warning("GOOGLE: –ö–∞–ø—á–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –≤—ã—Ö–æ–¥–∏–º")
                break
            
            # –ü–∞—Ä—Å–∏–º —Å—Å—ã–ª–∫–∏
            selectors = [
                "a[data-sokoban-click]",
                "div.g a",
                "h3 a",
            ]
            
            links_on_page = set()
            for selector in selectors:
                try:
                    elems = page.locator(selector)
                    count = await elems.count()
                    
                    for i in range(count):
                        try:
                            href = await elems.nth(i).get_attribute("href")
                            # Google –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å /url?q=... –ø–∞—Ä–∞–º–µ—Ç—Ä
                            if "/url?q=" in str(href):
                                href = str(href).split("/url?q=")[1].split("&")[0]
                            
                            cleaned = clean_url(href)
                            if cleaned:
                                links_on_page.add(cleaned)
                        except:
                            pass
                except:
                    pass
            
            collected_links.update(links_on_page)
            logging.info(f"GOOGLE: –ù–∞–π–¥–µ–Ω–æ {len(links_on_page)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é
            if page_num < pages:
                try:
                    next_btn = page.locator("a#pnnext")
                    if await next_btn.count() > 0:
                        await human_pause(4, 10)
                        await next_btn.click()
                        await wait_for_page_load(page)
                    else:
                        logging.info("GOOGLE: –ù–µ—Ç –∫–Ω–æ–ø–∫–∏ '–î–∞–ª–µ–µ', –≤—ã—Ö–æ–¥–∏–º")
                        break
                except Exception as e:
                    logging.error(f"GOOGLE: –û—à–∏–±–∫–∞ –∫–ª–∏–∫–∞ - {e}")
                    break
        
        except Exception as e:
            logging.error(f"GOOGLE: –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} - {e}")
            continue
    
    logging.info(f"GOOGLE: –ò—Ç–æ–≥–æ {len(collected_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")


# ================ MAIN ================

async def search_suppliers(
    query: str,
    pages: int = 3,
    use_proxy: Optional[str] = None,
) -> Set[str]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–¥–ª—è FastAPI integration)
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä. "–¢—Ä—É–±–∞ –ü–ù–î –∫—É–ø–∏—Ç—å")
        pages: –ö–æ–ª-–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (default 3)
        use_proxy: Optional proxy (http://proxy:port)
    
    Returns:
        Set —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—á–∏—â–µ–Ω–Ω—ã—Ö URL
    """
    
    collected_links = set()
    
    async with async_playwright() as p:
        try:
            # –ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å –ø—Ä–æ–∫—Å–∏)
            launch_args = {}
            if use_proxy:
                launch_args["proxy"] = {"server": use_proxy}
            
            browser = await p.chromium.launch(**launch_args)
            context = await browser.new_context(
                user_agent=random.choice(USER_AGENTS),
            )
            
            y_page = await context.new_page()
            g_page = await context.new_page()
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            await asyncio.gather(
                parse_yandex(y_page, query, pages, collected_links),
                parse_google(g_page, query, pages, collected_links),
            )
            
            await browser.close()
        
        except Exception as e:
            logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    
    logging.info(f"‚úÖ –§–∏–Ω–∞–ª: {len(collected_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
    return collected_links


async def main():
    """–î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –∏–∑ –∫–æ–Ω—Å–æ–ª–∏"""
    query = input("–í–≤–µ–¥–∏ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä. '–¢—Ä—É–±–∞ –ü–ù–î –∫—É–ø–∏—Ç—å'): ").strip()
    pages = int(input("–ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞ (—Å—Ç—Ä–∞–Ω–∏—Ü—ã, default 3): ") or "3")
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥: '{query}' ({pages} —Å—Ç—Ä–∞–Ω–∏—Ü)...\n")
    
    results = await search_suppliers(query, pages)
    
    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Å—Å—ã–ª–æ–∫:\n")
    for url in sorted(results):
        print(url)


if __name__ == "__main__":
    asyncio.run(main())
